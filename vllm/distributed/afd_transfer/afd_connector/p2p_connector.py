# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: Copyright contributors to the vLLM project

import re
from datetime import timedelta

import torch
from torch.distributed.distributed_c10d import (
    _update_default_pg,
    _get_default_group,
)
from typing import Any, Optional

from .base import AFDConnectorBase
from .metadata import AFDConnectorMetadata
from vllm.distributed.parallel_state import (
    init_afd_process_group,
    init_model_parallel_group,
)
from vllm.sequence import IntermediateTensors
from vllm.logger import init_logger
from vllm.config import VllmConfig
from vllm.platforms import current_platform

logger = init_logger(__name__)


class DefaultProcessGroupSwitcher:
    def __init__(self, default_group, new_default_group):
        self.default_group = default_group
        self.new_default_group = new_default_group

    def __enter__(self):
        _update_default_pg(self.new_default_group)

    def __exit__(self, exc_type, exc_value, traceback):
        _update_default_pg(self.default_group)


class P2PAFDConnector(AFDConnectorBase):
    def __init__(
        self,
        rank: int,
        local_rank: int,
        config: "VllmConfig",
    ) -> None:
        self.rank = rank
        self.local_rank = local_rank
        self._initialized = False
        self.config = config

    def close(self) -> None:
        """Close the connector and release resources."""
        # destroy process group
        pass

    def init_afd_connector(self) -> None:
        """Initialize the AFD connector."""
        afd_size = self.config.afd_config.afd_extra_config.get("afd_size")
        role = self.config.afd_config.afd_role
        host = self.config.afd_config.afd_host
        port = self.config.afd_config.afd_port
        attn_size, ffn_size = map(
            int, re.match(r"(\d+)\D+(\d+)", afd_size).groups()
        )
        world_rank = self.rank if role == "attention" else self.rank + attn_size

        logger.info(
            "world_size = %d, world_rank = %d", ffn_size + attn_size, world_rank
        )
        backend = current_platform.dist_backend
        afd_pg = init_afd_process_group(
            backend=backend,
            init_method=f"tcp://{host}:{port}",
            world_size=ffn_size + attn_size,
            rank=world_rank,
            group_name="afd",
            timeout=timedelta(minutes=2),
        )
        ffn_ranks = [i for i in range(ffn_size, ffn_size + attn_size)]
        attn_ranks = [i for i in range(attn_size)]

        default_pg_switcher = DefaultProcessGroupSwitcher(
            _get_default_group(), afd_pg
        )
        with default_pg_switcher:
            sub_group_ranks = []
            for i in range(len(ffn_ranks)):
                ranks = list([attn_ranks[i], ffn_ranks[i]])
                sub_group_ranks.append(ranks)
            self.process_group = init_model_parallel_group(
                sub_group_ranks, self.rank, backend=backend, group_name="ae"
            )

        logger.info("p2p connector initialized")

        self._initialized = True

    def is_initialized(self) -> bool:
        """Check if the connector is initialized and ready to use.

        Returns:
            bool: True if the connector is initialized, False otherwise.
        """
        return self._initialized

    def send_attn_output(
        self,
        hidden_states: torch.Tensor,
        metadata: "AFDConnectorMetadata",
    ) -> Any:
        """
        This method will be called by the ATTN side.


        * To send the intermediate tensors generated by ATTN instances to FFN.
        """

        intermediate_tensors = IntermediateTensors(
            {
                "hidden_states": hidden_states,
            }
        )
        try:
            self.process_group.send_tensor_dict(
                intermediate_tensors.tensors,
                all_gather_group=None,
            )
            dst = (
                self.process_group.rank_in_group + 1
            ) % self.process_group.world_size
            self.process_group.send_object(metadata, dst)
        except Exception as e:
            raise RuntimeError(f"Communication error: {e}")

    def recv_attn_output(
        self,
        timeout_ms: Optional[int] = None,
    ) -> tuple[torch.Tensor, "AFDConnectorMetadata"]:
        """
        This method will be called by the FFN side.


        * To receive the intermediate tensors from ATTN.
        * And (Maybe) dispatch them from the receiver to other GPUs.
        """
        intermediate_tensors = self.process_group.recv_tensor_dict(
            all_gather_group=None,
        )
        src = (
            self.process_group.rank_in_group - 1
        ) % self.process_group.world_size
        metadata = self.process_group.recv_object(src)
        return intermediate_tensors["hidden_states"], metadata

    # -------------------------------------------------------------------------
    #                                attn <- ffn
    # -------------------------------------------------------------------------
    def send_ffn_output(
        self,
        ffn_output: torch.Tensor,
        metadata: "AFDConnectorMetadata",
    ) -> None:
        """
        This method will be called by the FFN side.


        * To send the intermediate tensors generated by FFN instances back to
            the sender (this should be the same GPU as it comes from)
        """
        intermediate_tensors = IntermediateTensors(
            {
                "hidden_states": ffn_output,
            }
        )
        self.process_group.send_tensor_dict(
            intermediate_tensors.tensors,
        )
        dst = (
            self.process_group.rank_in_group + 1
        ) % self.process_group.world_size

        self.process_group.send_object(metadata, dst)

    def recv_ffn_output(
        self,
        handle: Any,
    ) -> torch.Tensor:
        """
        This method will be called by the ATTN side.


        * To receive the MOE output intermediate tensors.
        * And (Maybe) dispatch them from the receiver to other GPUs.
            (this should be the same GPU as it comes from)
        """
        intermediate_tensors = self.process_group.recv_tensor_dict(
            all_gather_group=None,
        )
        src = (
            self.process_group.rank_in_group - 1
        ) % self.process_group.world_size

        self.process_group.recv_object(src)
        return intermediate_tensors["hidden_states"]
